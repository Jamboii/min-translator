{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import Tokenizer, normalize_text\n",
    "from src.vis import plot_attention\n",
    "\n",
    "from typing import Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jupyter_black\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "jupyter_black.load()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tok: Tokenizer = torch.load(\"data/src_tok.pt\")\n",
    "tgt_tok: Tokenizer = torch.load(\"data/tgt_tok.pt\")\n",
    "vocab_size = src_tok.vocab_size\n",
    "pad_token_ix = src_tok.wtoi[src_tok.pad_token]\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Pad token index: {pad_token_ix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl: DataLoader = torch.load(\"data/train_dl.pt\")\n",
    "val_dl: DataLoader = torch.load(\"data/val_dl.pt\")\n",
    "test_dl: DataLoader = torch.load(\"data/test_dl.pt\")\n",
    "tiny_train_dl: DataLoader = torch.load(\"data/tiny_train_dl.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        :param queries: (N, M, D) where M is the number of queries and D is the dimension of the query\n",
    "        :param keys: (N, T, D) where T is the number of keys\n",
    "        :param values: (N, T, V) where V is the dimension of the value\n",
    "        :param mask: (N, M, T) where T is the number of keys\n",
    "        :return: (N, M, V)\n",
    "        \"\"\"\n",
    "        _, _, D = queries.shape\n",
    "        # (N, M, D) @ (N, D, T) -> (N, M, T)\n",
    "        self.logits = queries @ keys.transpose(1, 2) * D**-0.5\n",
    "        # Mask logits at indices < valid lens\n",
    "        if mask is not None:\n",
    "            self.logits = self.logits.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        self.attn_weights = F.softmax(self.logits, dim=-1)  # (N, M, T)\n",
    "        self.attn_weights = self.dropout(self.attn_weights)\n",
    "        return self.attn_weights @ values  # (N, M, T) @ (N, T, V) -> (N, M, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, T, D, V = 2, 3, 4, 5, 6\n",
    "queries = torch.ones(N, M, D)\n",
    "keys = torch.ones(N, T, D)\n",
    "values = torch.ones(N, T, V)\n",
    "\n",
    "attn = ScaledDotProductAttention()\n",
    "context = attn.forward(queries, keys, values)\n",
    "context.shape, context.shape == (N, M, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "emb_ = nn.Embedding(vocab_size, 16).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "attn = ScaledDotProductAttention(dropout=0.0)\n",
    "context = attn.forward(emb, emb, emb)\n",
    "context.shape, context.shape == emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 2\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=attn.attn_weights[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Self-Attention\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "emb_ = nn.Embedding(vocab_size, 16).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "attn = ScaledDotProductAttention()\n",
    "context = attn.forward(emb, emb, emb, mask=mask)\n",
    "context.shape, context.shape == emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 2\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=attn.attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param emb_size: dimension of the token embeddings\n",
    "        :param head_size: dimension of each attention head\n",
    "        :param num_heads: number of heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.E = num_heads\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        # Input projection into attention\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.Q = nn.Linear(emb_size, self.hidden_size, bias=False)\n",
    "        self.K = nn.Linear(emb_size, self.hidden_size, bias=False)\n",
    "        self.V = nn.Linear(emb_size, self.hidden_size, bias=False)\n",
    "        # Output projection\n",
    "        self.O = nn.Linear(self.hidden_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def transpose_input(self, X):\n",
    "        \"\"\"\n",
    "        Transpose QKV inputs to fit number of heads\n",
    "        :param X: (N, T, H) where T is the number of tokens and H is the dimension of the token\n",
    "        :return: (N*E, T, H//E)\n",
    "        \"\"\"\n",
    "        E = self.E\n",
    "        N, T, H = X.shape\n",
    "        X = X.reshape(N, T, E, -1)  # (N, T, E, H // E)\n",
    "        X = X.transpose(1, 2)  # (N, E, T, H//E)\n",
    "        X = X.reshape(N * E, T, -1)  # (N*E, T, H//E)\n",
    "        return X\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"\n",
    "        Transpose attention output back to original\n",
    "        shape\n",
    "        :param X: (N*E, T, H//E)\n",
    "        :return: (N, T, H)\n",
    "        \"\"\"\n",
    "        E = self.E\n",
    "        NE, T, H_E = X.shape\n",
    "        X = X.reshape(-1, E, T, H_E)  # (N, E, T, H//E)\n",
    "        X = X.transpose(1, 2)  # (N, T, E, H//E)\n",
    "        X = X.reshape(-1, T, H_E * E)  # (N, T, H)\n",
    "        return X\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        :param queries: (N, M, D) where M is the number of queries and H is the dimension of the query\n",
    "        :param keys: (N, T, D) where T is the number of keys\n",
    "        :param values: (N, T, D)\n",
    "        :param mask: (N, M, T)\n",
    "        \"\"\"\n",
    "        # Project QKV into matrices which can fit into each attention head\n",
    "        query = self.transpose_input(\n",
    "            self.Q(queries)\n",
    "        )  # (N, M, D) -> (N, M, H) -> (N*E, M, H//E)\n",
    "        key = self.transpose_input(\n",
    "            self.K(keys)\n",
    "        )  # (N, T, D) -> (N, T, H) -> (N*E, T, H//E)\n",
    "        value = self.transpose_input(self.V(values))  # (N*E, T, H//E)\n",
    "\n",
    "        # Copy mask for all heads\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat_interleave(self.E, dim=0)  # (N*E, M, T)\n",
    "\n",
    "        # Now we can fit E copies of our N samples through a single attention head\n",
    "        # and have it essentially emulate having multiple attention heads\n",
    "        context = self.attention.forward(\n",
    "            queries=query,\n",
    "            keys=key,\n",
    "            values=value,\n",
    "            mask=mask,\n",
    "        )  # (N*E, M, H//E)\n",
    "\n",
    "        # Concatenate head outputs back into a single tensor, pass through Linear layer\n",
    "        out = self.transpose_output(context)  # (N, M, H)\n",
    "        return self.dropout(self.O(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, T = 2, 3, 4\n",
    "H, E = 8, 2\n",
    "queries = torch.ones(N, M, H)\n",
    "keys = torch.ones(N, T, H)\n",
    "values = torch.ones(N, T, H)\n",
    "\n",
    "attn = MultiHeadAttention(emb_size=H, head_size=H, num_heads=E)\n",
    "context = attn.forward(queries, keys, values)\n",
    "context.shape, context.shape == (N, M, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "H, E = 16, 4\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)\n",
    "\n",
    "emb_ = nn.Embedding(vocab_size, H).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "attn = MultiHeadAttention(emb_size=H, head_size=H, num_heads=E).to(device)\n",
    "context = attn.forward(emb, emb, emb, mask=mask)\n",
    "context.shape, context.shape == emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = attn.attention.attn_weights.view(-1, E, T, T)  # (N, E, M, T)\n",
    "ix = 0\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout, max_len=1000):\n",
    "        \"\"\"\n",
    "        hidden_size: dimension `d_model` with indices `i`\n",
    "        max_len: position `pos`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = hidden_size\n",
    "        self.pos = max_len\n",
    "        # Our positional encoding tensor\n",
    "        self.P = torch.zeros(1, self.pos, self.d_model)\n",
    "        # Declare the inputs to the trig functions of the pos encoding\n",
    "        self.numerator = torch.arange(self.pos, dtype=torch.float32).unsqueeze(\n",
    "            1\n",
    "        )  # (I, 1)\n",
    "        self.denominator = torch.pow(\n",
    "            10000,\n",
    "            torch.arange(0, self.d_model, 2, dtype=torch.float32) / self.d_model,\n",
    "        )\n",
    "        X = self.numerator / self.denominator\n",
    "        # Assign trig functions to alternating even and odd dimensions of each position\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "        # Set dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (N, T, D)\n",
    "        We add the positional encoding to each sample of X\n",
    "        \"\"\"\n",
    "        _, T, _ = X.shape\n",
    "        # Limit our addition of the positional encoding up to the timestep dimension T\n",
    "        X += self.P[:, :T].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, T = 32, 60\n",
    "dropout = 0.1\n",
    "pos_encoding = PositionalEncoding(hidden_size=D, dropout=dropout)\n",
    "X = pos_encoding(torch.zeros(1, T, D))\n",
    "P = pos_encoding.P[:, :T]\n",
    "X.shape, P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "H, E = 16, 4\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)\n",
    "\n",
    "emb_ = nn.Embedding(vocab_size, H).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "pos_ = PositionalEncoding(hidden_size=H, dropout=0.1).to(device)\n",
    "emb_and_pos = pos_(emb)\n",
    "\n",
    "attn = MultiHeadAttention(emb_size=H, head_size=H, num_heads=E).to(device)\n",
    "context = attn.forward(emb_and_pos, emb_and_pos, emb_and_pos, mask=mask)\n",
    "context.shape, context.shape == emb_and_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = attn.attention.attn_weights.view(-1, E, T, T)  # (N, E, M, T)\n",
    "ix = 0\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (N, T, D)\n",
    "        output: (N, T, O)\n",
    "        \"\"\"\n",
    "        # lin1 -> relu -> lin2\n",
    "        return self.net(X)  # (N, T, O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual + Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: (N, T, H)\n",
    "        Y: (N, T, H)\n",
    "        Dropout -> Residual addition -> Layer norm\n",
    "        \"\"\"\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Residual + Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, H = 2, 3, 4\n",
    "an = AddNorm(H, 0.5)\n",
    "X, Y = torch.ones(N, T, H), torch.ones(N, T, H)\n",
    "out = an(X, Y)\n",
    "out.shape, out.shape == (N, T, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "H, E = 16, 4\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)\n",
    "\n",
    "emb_ = nn.Embedding(vocab_size, H).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "pos_ = PositionalEncoding(hidden_size=H, dropout=0.1).to(device)\n",
    "emb_and_pos = pos_(emb)\n",
    "\n",
    "attn = MultiHeadAttention(emb_size=H, head_size=H, num_heads=E).to(device)\n",
    "context = attn.forward(emb_and_pos, emb_and_pos, emb_and_pos, mask=mask)\n",
    "print(context.shape, context.shape == emb_and_pos.shape)\n",
    "\n",
    "an = AddNorm(H, 0.5).to(device)\n",
    "out = an(emb_and_pos, context)\n",
    "print(out.shape, out.shape == emb_and_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "- Multi-head self-attention\n",
    "- AddNorm\n",
    "- PositionWiseLinear\n",
    "- AddNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size,\n",
    "        head_size,\n",
    "        mlp_hidden_size,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param emb_size: dimension of the token embeddings\n",
    "        :param head_size: dimension of each attention head\n",
    "        :param mlp_hidden_size: hidden size of the Position-wise Linear block\n",
    "        :param num_heads: Number of attention heads\n",
    "        :param dropout: Dropout for AddNorm blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            emb_size=emb_size,\n",
    "            head_size=head_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.an1 = AddNorm(norm_shape=emb_size, dropout=dropout)\n",
    "        self.mlp = MLP(\n",
    "            input_size=emb_size, hidden_size=mlp_hidden_size, output_size=emb_size\n",
    "        )\n",
    "        self.an2 = AddNorm(norm_shape=emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        :param X: (N, T, D) emb + pos encoded tensor\n",
    "        :param mask: (N, T, T) mask tensor\n",
    "        :return: (N, T, D) output tensor\n",
    "        \"\"\"\n",
    "        Y = self.an1(X, self.attention.forward(X, X, X, mask))\n",
    "        return self.an2(Y, self.mlp.forward(Y))  # (N, T, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "D, H, E = 16, 32, 4\n",
    "dropout = 0.1\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "emb_ = nn.Embedding(vocab_size, D).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "pos_ = PositionalEncoding(hidden_size=D, dropout=dropout).to(device)\n",
    "emb_and_pos = pos_(emb)\n",
    "\n",
    "\"\"\"1 Encoder Block\"\"\"\n",
    "\n",
    "block = TransformerEncoderBlock(\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "enc_out = block.forward(emb_and_pos, mask=mask)\n",
    "enc_out.shape, enc_out.shape == (N, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = block.attention.attention.attn_weights.view(-1, E, T, T)  # (N, E, M, T)\n",
    "ix = 0\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "- Embedding + Positional Encoding\n",
    "- Nx Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_size: int,\n",
    "        head_size: int,\n",
    "        mlp_hidden_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the vocabulary\n",
    "        :param emb_size: dimension of the token embeddings\n",
    "        :param head_size: dimension of each attention head\n",
    "        :param mlp_hidden_size: hidden size of the Position-wise Linear block\n",
    "        :param num_heads: Number of attention heads\n",
    "        :param num_blocks: Number of encoder blocks\n",
    "        :param dropout: Dropout float value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D = emb_size\n",
    "        self.H = head_size\n",
    "        self.E = num_heads\n",
    "        self.B = num_blocks\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos = PositionalEncoding(emb_size, dropout)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\n",
    "                name=f\"block{i}\",\n",
    "                module=TransformerEncoderBlock(\n",
    "                    emb_size=emb_size,\n",
    "                    head_size=head_size,\n",
    "                    mlp_hidden_size=mlp_hidden_size,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                ),\n",
    "            )\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        :param X: (N, T) input tensor\n",
    "        :param mask: (N, T, T) mask tensor\n",
    "        \"\"\"\n",
    "        N, T = X.shape\n",
    "        # Rescale embedding values during positional encoding\n",
    "        # using sqrt of the fan-in\n",
    "        X = self.pos(self.emb(X) * self.D**0.5)  # (N, T, D)\n",
    "        self.attn_weights = torch.zeros((N, self.B, self.E, T, T), device=X.device)\n",
    "        # Send the embedded/encoded tensor through each encoder block\n",
    "        # The idea is that through each block, a more complex representation of the\n",
    "        # input samples is learned\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            X = block(X, mask)\n",
    "            self.attn_weights[\n",
    "                :, i : i + 1\n",
    "            ] = block.attention.attention.attn_weights.view(N, -1, T, T).unsqueeze(\n",
    "                1\n",
    "            )  # (N, 1, E, T, T)\n",
    "        return X  # (N, T, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "D, H, E, B, C = 16, 32, 4, 1, 2\n",
    "dropout = 0.1\n",
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "enc = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "enc_out = enc.forward(X, mask)\n",
    "enc_out.shape, enc_out.shape == (N, T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights are all uniform due to the gaussian weight initialization\n",
    "ix = 0\n",
    "sent = src_tok.untokenize(X[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=enc.attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "- Multi-head self-attention\n",
    "- AddNorm\n",
    "- Encoder-Decoder attention\n",
    "- AddNorm\n",
    "- MLP\n",
    "- AddNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tril + Padding Mask\n",
    "\n",
    "The decoder block embeddings need to be blind to any token embeddings in front of it,\n",
    "so a lower-triangular matrix is used as the mask.\n",
    "\n",
    "In addition, we still want to mask out any `<PAD>` tokens, so by taking the bitwise `AND`\n",
    "of both matrices, we keep both the autoregressive properties of the lower-triangular, and the\n",
    "masking of the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(tiny_train_dl))[0]\n",
    "N, T = X.shape\n",
    "tril = torch.tril(torch.ones(100, 100)).type(torch.long).to(device)\n",
    "mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "tril.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = tril[:T, :T] & mask\n",
    "new_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_ = nn.Embedding(vocab_size, H).to(device)\n",
    "emb = emb_(X)  # (N, T, D)\n",
    "\n",
    "wei = emb @ emb.transpose(-2, -1)\n",
    "wei = wei.masked_fill(new_mask == 0, float(\"-inf\"))\n",
    "wei[0, -5:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size,\n",
    "        head_size,\n",
    "        mlp_hidden_size,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param emb_size: dimension of the token embeddings\n",
    "        :param head_size: dimension of each attention head\n",
    "        :param mlp_hidden_size: hidden size of the Position-wise Linear block\n",
    "        :param num_heads: Number of attention heads\n",
    "        :param dropout: Dropout for AddNorm blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn1 = MultiHeadAttention(emb_size, head_size, num_heads, dropout)\n",
    "        self.an1 = AddNorm(emb_size, dropout)\n",
    "        self.attn2 = MultiHeadAttention(emb_size, head_size, num_heads, dropout)\n",
    "        self.an2 = AddNorm(emb_size, dropout)\n",
    "        self.mlp = MLP(emb_size, mlp_hidden_size, emb_size)\n",
    "        self.an3 = AddNorm(emb_size, dropout)\n",
    "        # Register a lower triangular mask for the decoder self-attention\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(100, 100)).type(torch.long))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X,\n",
    "        enc_out,\n",
    "        enc_mask=None,\n",
    "        dec_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param X: (N, T, D) emb + pos encoded tensor\n",
    "        :param enc_out: (N, T, D) Encoder block output\n",
    "        :param enc_mask: (N, T, T) mask tensor for enc-dec attention\n",
    "        :param dec_mask: (N, T, T) mask tensor for decoder block\n",
    "        :return: (N, T, D) output tensor\n",
    "        \"\"\"\n",
    "        # Get the valid lengths for the decoder sequence\n",
    "        if self.training:\n",
    "            _, T, _ = X.shape\n",
    "            # Each time step's valid length is only as long as the sequence up to that time step\n",
    "            dec_mask = self.tril[:T, :T] & dec_mask\n",
    "\n",
    "        # Masked self-attention\n",
    "        X2 = self.attn1.forward(X, X, X, dec_mask)\n",
    "        Y = self.an1(X, X2)  # (N, T, D)\n",
    "\n",
    "        # Encoder-decoder attention\n",
    "        if enc_out is not None:\n",
    "            Y2 = self.attn2.forward(Y, enc_out, enc_out, enc_mask)\n",
    "            Z = self.an2(Y, Y2)  # (N, T, D)\n",
    "        else:\n",
    "            Z = Y\n",
    "\n",
    "        return self.an3(Z, self.mlp(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Self-Attention\n",
    "D, H, E, B = 16, 32, 4, 1\n",
    "dropout = 0.1\n",
    "X, Y = next(iter(tiny_train_dl))\n",
    "N, T = X.shape\n",
    "enc_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "dec_mask = (Y != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "enc = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "enc_out = enc.forward(X, enc_mask)\n",
    "print(enc_out.shape, enc_out.shape == (N, T, D))\n",
    "\n",
    "\"\"\"Embedding + Positional Encoding\"\"\"\n",
    "\n",
    "emb_ = nn.Embedding(num_embeddings=V, embedding_dim=D).to(device)\n",
    "emb = emb_.forward(Y)  # (N, T, D)\n",
    "\n",
    "pos_ = PositionalEncoding(hidden_size=D, dropout=dropout).to(device)\n",
    "emb_and_pos = pos_.forward(emb)  # (N, T, D)\n",
    "\n",
    "\"\"\"1 Decoder Block\"\"\"\n",
    "\n",
    "block = TransformerDecoderBlock(\n",
    "    emb_size=D,\n",
    "    head_size=H,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "dec_out = block.forward(emb_and_pos, enc_out, enc_mask, dec_mask)\n",
    "print(dec_out.shape, dec_out.shape == (N, T, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn_weights = block.attn1.attention.attn_weights.view(-1, E, T, T)\n",
    "ix = 1\n",
    "sent = tgt_tok.untokenize(Y[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=self_attn_weights[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_attn_weights = block.attn2.attention.attn_weights.view(-1, E, T, T)\n",
    "ix = 1\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(sentence=src_sent, translation=tgt_sent, attention=ed_attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "- Embedding + Positional Encoding\n",
    "- Nx Decoder Blocks\n",
    "- Linear projection layer to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_size: int,\n",
    "        head_size: int,\n",
    "        mlp_hidden_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the vocabulary\n",
    "        :param emb_size: dimension of the token embeddings\n",
    "        :param head_size: dimension of each attention head\n",
    "        :param mlp_hidden_size: hidden size of the Position-wise Linear block\n",
    "        :param num_heads: Number of attention heads\n",
    "        :param num_blocks: Number of decoder blocks\n",
    "        :param dropout: Dropout float value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.D = emb_size\n",
    "        self.E = num_heads\n",
    "        self.B = num_blocks\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos = PositionalEncoding(emb_size, dropout)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\n",
    "                name=f\"block{i}\",\n",
    "                module=TransformerDecoderBlock(\n",
    "                    emb_size=emb_size,\n",
    "                    head_size=head_size,\n",
    "                    mlp_hidden_size=mlp_hidden_size,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                ),\n",
    "            )\n",
    "        self.lin = nn.Linear(emb_size, vocab_size)\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, X, enc_out, enc_mask=None, dec_mask=None):\n",
    "        \"\"\"\n",
    "        :param X: (N, T) input tensor\n",
    "        :param enc_out: (N, T, D) encoder output\n",
    "        :param enc_mask: (N, T, T) mask tensor for enc-dec attention\n",
    "        :param dec_mask: (N, T, T) mask tensor for decoder block\n",
    "        \"\"\"\n",
    "        N, T = X.shape\n",
    "        _, encT, _ = enc_out.shape\n",
    "        # Rescale embedding values during positional encoding\n",
    "        # using sqrt of the fan-in\n",
    "        X = self.pos(self.emb(X) * self.D**0.5)  # (N, T, D)\n",
    "\n",
    "        self.self_attn_weights = torch.zeros((N, self.B, self.E, T, T), device=X.device)\n",
    "        self.ed_attn_weights = torch.zeros(\n",
    "            (N, self.B, self.E, T, encT), device=X.device\n",
    "        )\n",
    "\n",
    "        # Send the embedded/encoded tensor through each encoder block\n",
    "        # The idea is that through each block, a more complex representation of the\n",
    "        # input samples is learned\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            X = block(X, enc_out, enc_mask, dec_mask)\n",
    "            self.self_attn_weights[\n",
    "                :, i : i + 1\n",
    "            ] = block.attn1.attention.attn_weights.view(N, -1, T, T).unsqueeze(\n",
    "                1\n",
    "            )  # self\n",
    "            self.ed_attn_weights[\n",
    "                :, i : i + 1\n",
    "            ] = block.attn2.attention.attn_weights.view(N, -1, T, encT).unsqueeze(\n",
    "                1\n",
    "            )  # enc-dec\n",
    "\n",
    "        return self.lin(X)  # (N, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "D, H, E, B = 16, 32, 4, 1\n",
    "dropout = 0.1\n",
    "X, Y = next(iter(tiny_train_dl))\n",
    "N, T = X.shape\n",
    "enc_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "dec_mask = (Y != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "enc = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "enc_out = enc.forward(X, enc_mask)\n",
    "print(enc_out.shape, enc_out.shape == (N, T, D))\n",
    "\n",
    "dec = TransformerDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "dec_out = dec.forward(Y, enc_out, enc_mask, dec_mask)\n",
    "print(dec_out.shape, dec_out.shape == (N, T, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.ed_attn_weights.shape, dec.self_attn_weights.shape  # (N, B, E, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(\n",
    "    sentence=src_sent, translation=tgt_sent, attention=dec.ed_attn_weights[ix]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1\n",
    "sent = tgt_tok.untokenize(Y[ix].detach().tolist(), remove_padding_tokens=False).split()\n",
    "plot_attention(sentence=sent, translation=sent, attention=dec.self_attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: TransformerEncoder,\n",
    "        decoder: TransformerDecoder,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param encoder: TransformerEncoder object\n",
    "        :param decoder: TransformerDecoder object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "        src_mask: torch.tensor = None,\n",
    "        tgt_mask: torch.tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param source: (N, T) input tensor\n",
    "        :param target: (N, T) target tensor\n",
    "        :param src_mask: (N, T, T) mask tensor for source\n",
    "        :param tgt_mask: (N, T, T) mask tensor for target\n",
    "        :return: (N, T, V) predictions\n",
    "        \"\"\"\n",
    "        # Encoder step\n",
    "        enc_out = self.enc.forward(source, src_mask)  # (N, T, D)\n",
    "        # Decoder step\n",
    "        outputs = dec.forward(target, enc_out, src_mask, tgt_mask)  # (N, T, V)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Self-Attention\n",
    "D, H, E, B = 16, 32, 4, 1\n",
    "dropout = 0.1\n",
    "X, Y = next(iter(tiny_train_dl))\n",
    "N, T = X.shape\n",
    "src_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "tgt_mask = (Y != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "enc = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "dec = TransformerDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    head_size=D,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "model = TransformerSeq2Seq(encoder=enc, decoder=dec).to(device)\n",
    "outputs = model.forward(X, Y, src_mask, tgt_mask)\n",
    "outputs.shape, outputs.shape == (N, T, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to Calculate a Loss Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(\n",
    "    sentence=sent, translation=sent, attention=model.dec.self_attn_weights[ix]\n",
    ")\n",
    "plot_attention(\n",
    "    sentence=src_sent, translation=tgt_sent, attention=model.dec.ed_attn_weights[ix]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs[:, :-1]\n",
    "targets = Y[:, 1:]\n",
    "logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(\n",
    "    model,\n",
    "    source,\n",
    "    target,\n",
    "    loss_fn,\n",
    "    pad_token: int = 0,\n",
    "):\n",
    "    # Forward pass - grab the logits that we'll map\n",
    "    # to probabilities in the loss calculation\n",
    "    src_mask = (source != pad_token).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "    tgt_mask = (target != pad_token).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "    logits = model.forward(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        src_mask=src_mask,\n",
    "        tgt_mask=tgt_mask,\n",
    "    )  # (N, T, V)\n",
    "    _, _, V = logits.shape\n",
    "    # Logits is the predictions for the next token given the previous tokens\n",
    "    logits = logits[:, :-1].reshape(-1, V)  # (N*(T-1), V)\n",
    "    # Targets are the next tokens in the sequence\n",
    "    target = target[:, 1:].reshape(-1)  # (N*(T-1),)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_fn(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optim,\n",
    "):\n",
    "    # Iterate through one epoch-worth of data\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through the data loader\n",
    "    for it, batch in enumerate(data_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Unpack the data loader\n",
    "        # into source and target sequences\n",
    "        xb, yb = batch\n",
    "\n",
    "        # Forward pass - grab the logits that we'll map\n",
    "        # to probabilities in the loss calculation\n",
    "        loss = model_forward(model, xb, yb, loss_fn, pad_token=pad_token_ix)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optim.step()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through all data in the data loader\n",
    "    for batch in data_loader:\n",
    "        # Unpack the data loader\n",
    "        xb, yb = batch\n",
    "\n",
    "        # Forward pass_to\n",
    "        loss = model_forward(model, xb, yb, loss_fn, pad_token=pad_token_ix)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, D, H = vocab_size, 256, D * 4\n",
    "E, B = 2, 2\n",
    "dropout = 0.1\n",
    "\n",
    "enc = TransformerEncoder(\n",
    "    vocab_size=V,\n",
    "    emb_size=D,\n",
    "    head_size=D // E,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ")\n",
    "dec = TransformerDecoder(\n",
    "    vocab_size=V,\n",
    "    emb_size=D,\n",
    "    head_size=D // E,\n",
    "    mlp_hidden_size=H,\n",
    "    num_heads=E,\n",
    "    num_blocks=B,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    encoder=enc,\n",
    "    decoder=dec,\n",
    ").to(device)\n",
    "model.train()\n",
    "\n",
    "epochs = 10\n",
    "print_every = 1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=10e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_tok.wtoi[tgt_tok.pad_token])\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_dl,\n",
    "        optim=optimizer,\n",
    "        loss_fn=criterion,\n",
    "    )\n",
    "    val_loss = evaluate_epoch(\n",
    "        model=model,\n",
    "        data_loader=val_dl,\n",
    "        loss_fn=criterion,\n",
    "    )\n",
    "    if val_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), \"best-model-transformer.pt\")\n",
    "        best_valid_loss = val_loss\n",
    "\n",
    "    if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "        print(\n",
    "            f\"({epoch+1}/{epochs})\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\",\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(f\"\\tValid Loss: {val_loss:7.3f} | Valid PPL: {np.exp(val_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best-model-transformer.pt\"))\n",
    "test_loss = evaluate_epoch(model, test_dl, criterion)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(tiny_train_dl))\n",
    "src_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "tgt_mask = (Y != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(\n",
    "        source=X,\n",
    "        target=Y,\n",
    "        src_mask=src_mask,\n",
    "        tgt_mask=tgt_mask,\n",
    "    )  # (N, T, V)\n",
    "model.train()\n",
    "\n",
    "ix = 1\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(\n",
    "    sentence=src_sent,\n",
    "    translation=src_sent,\n",
    "    attention=model.enc.attn_weights[ix],\n",
    "    title=\"Encoder Self-Attention\",\n",
    ")\n",
    "plot_attention(\n",
    "    sentence=tgt_sent,\n",
    "    translation=tgt_sent,\n",
    "    attention=model.dec.self_attn_weights[ix],\n",
    "    title=\"Decoder Self-Attention\",\n",
    ")\n",
    "plot_attention(\n",
    "    sentence=src_sent,\n",
    "    translation=tgt_sent,\n",
    "    attention=model.dec.ed_attn_weights[ix],\n",
    "    title=\"Encoder-Decoder Attention\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    src_tokenizer: Tokenizer,\n",
    "    tgt_tokenizer: Tokenizer,\n",
    "    device,\n",
    "    sos_token: str = \"<SOS>\",\n",
    "    eos_token: str = \"<EOS>\",\n",
    "    max_output_length: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    sentence: (T,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sentence = sentence.unsqueeze(0).to(device)  # (1, T)\n",
    "    src_mask = (sentence != pad_token_ix).type(torch.long).unsqueeze(1)  # (1, 1, T)\n",
    "    enc_out = model.enc(sentence, src_mask)  # (1, T, D)\n",
    "    X = torch.tensor(\n",
    "        [tgt_tokenizer.wtoi[sos_token]], dtype=torch.long, device=device\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    for i in range(max_output_length):\n",
    "        dec_out = model.dec(X, enc_out)  # (N, T, V)\n",
    "        logits = dec_out[:, -1]  # (N, V)\n",
    "\n",
    "        pred_token = logits.argmax(-1).reshape(1, -1)\n",
    "        X = torch.cat((X, pred_token), dim=1)  # (N, T+1)\n",
    "\n",
    "        if pred_token.item() == tgt_tokenizer.wtoi[eos_token]:\n",
    "            break\n",
    "\n",
    "    tokens = tgt_tok.untokenize(X.squeeze(0).tolist())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "src, tgt = X[ix], Y[ix]\n",
    "print(src_tok.untokenize(src.tolist()))\n",
    "print(tgt_tok.untokenize(tgt.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    src.to(device),\n",
    "    model,\n",
    "    src_tok,\n",
    "    tgt_tok,\n",
    "    device,\n",
    "    max_output_length=tgt_tok.max_length,\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sent = src_tok.untokenize(\n",
    "    src.detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "translation = translation.split(\" \")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(\n",
    "    sentence=src_sent,\n",
    "    translation=src_sent,\n",
    "    attention=model.enc.attn_weights[0],\n",
    "    title=\"Encoder Self-Attention\",\n",
    ")\n",
    "plot_attention(\n",
    "    sentence=translation,\n",
    "    translation=translation,\n",
    "    attention=model.dec.self_attn_weights[0],\n",
    "    title=\"Decoder Self-Attention\",\n",
    ")\n",
    "plot_attention(\n",
    "    sentence=src_sent,\n",
    "    translation=translation,\n",
    "    attention=model.dec.ed_attn_weights[0],\n",
    "    title=\"Encoder-Decoder Attention\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = torch.tensor([]).type(torch.long).to(device)\n",
    "Yte = torch.tensor([]).to(device)\n",
    "for data in test_dl:\n",
    "    Xte = torch.cat((Xte, data[0]), dim=0)\n",
    "    Yte = torch.cat((Yte, data[1]), dim=0)\n",
    "Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        src,\n",
    "        model,\n",
    "        src_tok,\n",
    "        tgt_tok,\n",
    "        device,\n",
    "        max_output_length=tgt_tok.max_length,\n",
    "    )\n",
    "    for src in tqdm(Xte)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\" \".join(t.split()[1:-1]) for t in translations]\n",
    "targets = [[\" \".join(tgt_tok.untokenize(t.tolist()).split()[1:-1])] for t in Yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "preds[0], targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=preds, references=targets, tokenizer=lambda x: x.split()\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
