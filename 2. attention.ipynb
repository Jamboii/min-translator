{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import Tokenizer, normalize_text\n",
    "from src.vis import plot_attention\n",
    "\n",
    "from typing import Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jupyter_black\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "jupyter_black.load()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tok: Tokenizer = torch.load(\"data/src_tok.pt\")\n",
    "tgt_tok: Tokenizer = torch.load(\"data/tgt_tok.pt\")\n",
    "vocab_size = src_tok.vocab_size\n",
    "pad_token_ix = src_tok.wtoi[src_tok.pad_token]\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Pad token index: {pad_token_ix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl: DataLoader = torch.load(\"data/train_dl.pt\")\n",
    "val_dl: DataLoader = torch.load(\"data/val_dl.pt\")\n",
    "test_dl: DataLoader = torch.load(\"data/test_dl.pt\")\n",
    "tiny_train_dl: DataLoader = torch.load(\"data/tiny_train_dl.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The Bahdanau Attention Model is structured as follows:\n",
    "- Bidirectional Encoder RNN\n",
    "- Unidirectional Decoder RNN with Bahdanau (Additive) Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(input_size, emb_size)  # (V, D)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, X) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param X: (N, T) where N is the batch size and T is the sequence length\n",
    "        :return out: (N, T, 2H) where H is the hidden size and 2 is for bidirectional\n",
    "        :return hidden: (L, N, 2H) where L is the number of layers\n",
    "        \"\"\"\n",
    "        emb = self.emb(X)  # (N, T, D)\n",
    "        out, hidden = self.rnn(emb)  # (N, T, 2H), (2L, N, H)\n",
    "        # Hidden states need to be concatenated on the hidden state axis\n",
    "        # AKA, go from (2L, N, H) -> (L, N, 2H)\n",
    "        hidden = torch.cat((hidden[:1], hidden[1:]), dim=-1)  # (L, N, 2H)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "X, _ = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "\n",
    "\"\"\"Testing the Encoder\"\"\"\n",
    "\n",
    "enc = AttnEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    ").to(device)\n",
    "out, hidden = enc(X)\n",
    "\n",
    "print(out.shape, out.shape == (*X.shape, 2 * H))  # (N, T, 2H)\n",
    "print(hidden.shape, hidden.shape == (1, X.shape[0], 2 * H))  # (L, N, 2H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau (Additive) Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_size: int,\n",
    "        key_size: int,\n",
    "        hidden_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(query_size, hidden_size, bias=False)  # (Dq, H)\n",
    "        self.K = nn.Linear(key_size, hidden_size, bias=False)  # (Dk, H)\n",
    "        self.V = nn.Linear(hidden_size, 1)  # (H, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        # Initialize weights\n",
    "        for name, param in module.named_parameters():\n",
    "            if name.startswith(\"V\"):\n",
    "                nn.init.zeros_(param.data)\n",
    "            else:\n",
    "                nn.init.normal_(param.data, mean=0.0, std=0.001**2)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        i = sequence word index (from decoder target sentence, aka queries)\n",
    "        j = annotation index (from encoder source sentence, aka keys and values)\n",
    "\n",
    "        e_{ij} = a(s_{i-1}, h_j) = v_a.T * tanh(W_a*s_{i-1} + U_a*h_j)\n",
    "        alpha_{ij} = softmax(e_{ij})\n",
    "        c_i = sum(alpha_{ij}*h_{j}) from j=1 to Tx\n",
    "\n",
    "        queries: (N, M, Dq)\n",
    "        keys: (N, T, Dk)\n",
    "        values: (N, T, Dv)\n",
    "        \"\"\"\n",
    "\n",
    "        def a(s, h):\n",
    "            # Compute query aka W_a * s_{i-1} and key aka U_a * h_j\n",
    "            query = self.Q(s)  # (N, M, Dq) @ (Dq, H) -> (N, M, H)\n",
    "            key = self.K(h)  # (N, T, Dk) @ (Dk, HJ -> (N, T, H)\n",
    "\n",
    "            # Now we need to add and take the tanh of them\n",
    "            query_key = query.unsqueeze(2) + key.unsqueeze(1)  # (N, M, T, H)\n",
    "            return self.V(torch.tanh(query_key)).squeeze(\n",
    "                -1\n",
    "            )  # (N, M, T, H) @ (H, 1) -> (N, M, T)\n",
    "\n",
    "        # Calculate e_ij\n",
    "        e_ij = a(queries, keys)\n",
    "\n",
    "        # Now we calculate the attention weights alpha_{ij}\n",
    "        if mask is not None:\n",
    "            e_ij = e_ij.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        self.attn_weights = F.softmax(e_ij, dim=-1)  # (N, M, T)\n",
    "\n",
    "        # To calculate the full context vector c_i, we matmul the attention weights\n",
    "        # with the annotations h_j aka the values\n",
    "        context = self.attn_weights @ values  # (N, M, T) @ (N, T, Dv) -> N, M, Dv\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "X, Y = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "enc_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "\"\"\"Testing the Encoder\"\"\"\n",
    "\n",
    "enc = AttnEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    ").to(device)\n",
    "out, hidden = enc(X)\n",
    "\n",
    "print(out.shape, out.shape == (*X.shape, 2 * H))  # (N, T, 2H)\n",
    "print(hidden.shape, hidden.shape == (1, X.shape[0], 2 * H))  # (L, N, 2H)\n",
    "\n",
    "\"\"\"Testing Additive Attention\"\"\"\n",
    "\n",
    "# T = Y.shape[1]\n",
    "T = 1\n",
    "emb_ = nn.Embedding(vocab_size, D).to(device)\n",
    "dec_in = emb_(Y[:, :T])  # (N, 1, D)\n",
    "\n",
    "\n",
    "attn = AdditiveAttention(\n",
    "    query_size=H,\n",
    "    key_size=2 * H,\n",
    "    hidden_size=H,\n",
    ").to(device)\n",
    "\n",
    "context = attn(dec_in, out, out, mask=enc_mask)\n",
    "print(context.shape, context.shape == (X.shape[0], T, 2 * H))  # (N, 1, 2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 2\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()[:T]\n",
    "plot_attention(sentence=src_sent, translation=tgt_sent, attention=attn.attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: AdditiveAttention,\n",
    "        output_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.attn = attention\n",
    "        self.emb = nn.Embedding(output_size, emb_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_size + hidden_size,  # D+2H\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lin = nn.Linear(emb_size + 2 * hidden_size, output_size)  # D+3H\n",
    "\n",
    "    def forward(self, Y, enc_out, hidden, mask=None):\n",
    "        \"\"\"\n",
    "        :param Y: (N, 1) target samples for some time step t\n",
    "        :param enc_out: (N, T, H) encoder hidden state\n",
    "        :param hidden: (1, N, H) decoder hidden state\n",
    "        - the hidden parameter is initially the final encoder hidden state\n",
    "        :param mask: (N, 1, T) mask for the encoder output\n",
    "        :return out: (N, 1, V) output for time step t\n",
    "        \"\"\"\n",
    "        emb = self.emb(Y)  # (N, 1, D)\n",
    "        query_in = hidden.permute(1, 0, 2)  # (N, 1, H)\n",
    "        context = self.attn.forward(\n",
    "            queries=query_in,\n",
    "            keys=enc_out,\n",
    "            values=enc_out,\n",
    "            mask=mask,\n",
    "        )  # (N, 1, H)\n",
    "\n",
    "        # Since it's complicated to open up the GRU and add the context vector\n",
    "        # in its computations, we can just concatenate the context vector to\n",
    "        # the embedded target input\n",
    "        emb_and_context = torch.cat((emb, context), dim=-1)  # (N, 1, D+H)\n",
    "\n",
    "        # Get the RNN output and final hidden state\n",
    "        dec_out, hidden = self.rnn.forward(\n",
    "            emb_and_context, hidden\n",
    "        )  # (N, 1, H), (1, N, H)\n",
    "        assert (dec_out == hidden.permute(1, 0, 2)).all()\n",
    "\n",
    "        # Now, concatenate the decoder output s_{i-1}, the embedding Ey_{i-1} and\n",
    "        # the context vector c_i, and pass them through a linear layer\n",
    "        lin_in = torch.cat((emb_and_context, dec_out), dim=-1)  # (N, 1, D+2H)\n",
    "        out = self.lin(lin_in)  # (N, 1, V)\n",
    "        return out, hidden, self.attn.attn_weights  # (N, 1, V), (1, N, H), (N, 1, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "X, Y = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "enc_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "\"\"\"Testing the Encoder\"\"\"\n",
    "\n",
    "enc = AttnEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H // 2,\n",
    ").to(device)\n",
    "out, hidden = enc(X)\n",
    "\n",
    "print(out.shape, out.shape == (*X.shape, H))  # (N, T, 2H)\n",
    "print(hidden.shape, hidden.shape == (1, X.shape[0], H))  # (L, N, 2H)\n",
    "\n",
    "\"\"\"Testing the Decoder\"\"\"\n",
    "\n",
    "attn = AdditiveAttention(\n",
    "    query_size=H,\n",
    "    key_size=H,\n",
    "    hidden_size=H,\n",
    ").to(device)\n",
    "\n",
    "dec = AttnDecoder(\n",
    "    attention=attn,\n",
    "    output_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    ").to(device)\n",
    "\n",
    "T = 1\n",
    "out, hidden, attn_weights = dec(Y[:, :T], out, hidden, mask=enc_mask)\n",
    "print(out.shape, out.shape == (X.shape[0], 1, vocab_size))  # (N, 1, V)\n",
    "print(hidden.shape, hidden.shape == (1, X.shape[0], H))  # (1, N, H)\n",
    "print(\n",
    "    attn_weights.shape, attn_weights.shape == (X.shape[0], 1, X.shape[1])\n",
    ")  # (N, 1, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 2\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()[:T]\n",
    "plot_attention(sentence=src_sent, translation=tgt_sent, attention=attn_weights[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Seq2Seq Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnSeq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: AttnEncoder,\n",
    "        decoder: AttnDecoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        # Initialize uniform weights between -0.08 and 0.08\n",
    "        # for the model\n",
    "        for _, param in module.named_parameters():\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "        teacher_force_ratio: float = 0.0,\n",
    "        mask: torch.tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param source: (N, T) where N is the batch size and T is the sequence length\n",
    "        :param target: (N, T)\n",
    "        :return: (N, T, V) predictions for each token in the target sequence\n",
    "        \"\"\"\n",
    "        N, T = source.shape\n",
    "        V = self.dec.output_size\n",
    "        # Encoder step\n",
    "        enc_out, hidden = self.enc.forward(source)  # (N, T, 2H), (1, N, 2H)\n",
    "        # Decoder step\n",
    "        self.attn_weights = torch.zeros(N, T, T, device=source.device)  # (N, T, T)\n",
    "        outputs = torch.zeros(N, T, V, device=source.device)  # (N, T, V)\n",
    "        target_t = target[:, :1]  # (N, 1) initial decoder input token\n",
    "\n",
    "        # We loop here as to let the function decide which input to use in each proceeding\n",
    "        # RNN cell\n",
    "        for t in range(1, T):\n",
    "            dec_out, hidden, attn = self.dec.forward(\n",
    "                Y=target_t,\n",
    "                enc_out=enc_out,\n",
    "                hidden=hidden,\n",
    "                mask=mask,\n",
    "            )  # (N, 1, V), (1, N, 2H)\n",
    "            # Set attention output into total attention\n",
    "            self.attn_weights[:, t : t + 1, :] = attn\n",
    "            # Set decoder output into total outputs\n",
    "            outputs[:, t : t + 1] = dec_out  # (N, 1, V) -> (N, T, V)\n",
    "\n",
    "            # Set up next input to decoder\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            target_t = target[:, t : t + 1] if teacher_force else dec_out.argmax(-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Attention Seq2Seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "X, Y = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "enc_mask = (X != pad_token_ix).type(torch.long).unsqueeze(1)  # (N, 1, T)\n",
    "\n",
    "enc = AttnEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H // 2,\n",
    ")\n",
    "\n",
    "attn = AdditiveAttention(\n",
    "    query_size=H,\n",
    "    key_size=H,\n",
    "    hidden_size=H,\n",
    ")\n",
    "\n",
    "dec = AttnDecoder(\n",
    "    attention=attn,\n",
    "    output_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    ")\n",
    "\n",
    "model = AttnSeq2Seq(\n",
    "    encoder=enc,\n",
    "    decoder=dec,\n",
    ").to(device)\n",
    "out = model.forward(X, Y, teacher_force_ratio=0.0, mask=enc_mask)\n",
    "print(out.shape, out.shape == (*Y.shape, vocab_size))  # (N, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 2\n",
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "tgt_sent = tgt_tok.untokenize(\n",
    "    Y[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(\n",
    "    sentence=src_sent, translation=tgt_sent, attention=model.attn_weights[ix]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Attention Seq2Seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(\n",
    "    model,\n",
    "    source,\n",
    "    target,\n",
    "    loss_fn,\n",
    "    teacher_force_ratio,\n",
    "):\n",
    "    # Forward pass - grab the logits that we'll map\n",
    "    # to probabilities in the loss calculation\n",
    "    logits = model.forward(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        teacher_force_ratio=teacher_force_ratio,\n",
    "    )  # (N, T, V)\n",
    "    _, _, V = logits.shape\n",
    "    # Fit the logits into 2 dimensions\n",
    "    logits = logits[:, 1:].reshape(-1, V).to(device)  # (N*(T-1), V)\n",
    "    target = target[:, 1:].reshape(-1)  # (N*(T-1),)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_fn(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optim,\n",
    "    teacher_force_ratio: float = 0.5,\n",
    "):\n",
    "    # Iterate through one epoch-worth of data\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through the data loader\n",
    "    for it, batch in enumerate(data_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Unpack the data loader\n",
    "        # into source and target sequences\n",
    "        xb, yb = batch  # (N, T), (N, T)\n",
    "\n",
    "        # Forward pass - grab the logits that we'll map\n",
    "        # to probabilities in the loss calculation\n",
    "        loss = model_forward(model, xb, yb, loss_fn, teacher_force_ratio)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optim.step()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through all data in the data loader\n",
    "    for batch in data_loader:\n",
    "        # Unpack the data loader\n",
    "        xb, yb = batch\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model_forward(model, xb, yb, loss_fn, teacher_force_ratio=0.0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, H, L = 256, 512, 2\n",
    "lr = 1e-3\n",
    "\n",
    "enc = AttnEncoder(vocab_size, D, H // 2)\n",
    "attn = AdditiveAttention(H, H, H)\n",
    "dec = AttnDecoder(attn, vocab_size, D, H)\n",
    "model = AttnSeq2Seq(enc, dec).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_tok.wtoi[tgt_tok.pad_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "teacher_force_ratio = 0.5\n",
    "\n",
    "epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_dl,\n",
    "        optim=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        teacher_force_ratio=teacher_force_ratio,\n",
    "    )\n",
    "    val_loss = evaluate_epoch(\n",
    "        model=model,\n",
    "        data_loader=val_dl,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    if val_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), \"best-model-attn.pt\")\n",
    "        best_valid_loss = val_loss\n",
    "    print(\n",
    "        f\"({epoch+1}/{epochs})\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\",\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(f\"\\tValid Loss: {val_loss:7.3f} | Valid PPL: {np.exp(val_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best-model-attn.pt\"))\n",
    "test_loss = evaluate_epoch(model, test_dl, loss_fn)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    src_tokenizer: Tokenizer,\n",
    "    tgt_tokenizer: Tokenizer,\n",
    "    device,\n",
    "    sos_token: str = \"<SOS>\",\n",
    "    eos_token: str = \"<EOS>\",\n",
    "    max_output_length: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    sentence: (T,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sentence = sentence.unsqueeze(0).to(device)  # (1, T)\n",
    "    enc_out, hidden = model.enc(sentence)  # (1, T, D)\n",
    "\n",
    "    X = torch.tensor(\n",
    "        [tgt_tokenizer.wtoi[sos_token]], dtype=torch.long, device=device\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    attentions = torch.zeros(1, max_output_length, max_output_length).to(device)\n",
    "    for i in range(max_output_length):\n",
    "        dec_out, hidden, attn = model.dec(X[:, -1:], enc_out, hidden)  # (N, T, V)\n",
    "        attentions[:, i : i + 1] = attn\n",
    "        logits = dec_out[:, -1]  # (N, V)\n",
    "\n",
    "        pred_token = logits.argmax(-1).reshape(1, -1)\n",
    "        X = torch.cat((X, pred_token), dim=1)  # (N, T+1)\n",
    "\n",
    "        if pred_token.item() == tgt_tokenizer.wtoi[eos_token]:\n",
    "            break\n",
    "\n",
    "    tokens = tgt_tok.untokenize(X.squeeze(0).tolist())\n",
    "\n",
    "    return tokens, attentions[:, : len(tokens.split(\" \")) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 3\n",
    "src, tgt = X[ix], Y[ix]\n",
    "print(src_tok.untokenize(src.tolist()))\n",
    "print(tgt_tok.untokenize(tgt.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attn = translate_sentence(\n",
    "    src.to(device),\n",
    "    model,\n",
    "    src_tok,\n",
    "    tgt_tok,\n",
    "    device,\n",
    "    max_output_length=tgt_tok.max_length,\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translation.split()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sent = src_tok.untokenize(\n",
    "    X[ix].detach().tolist(), remove_padding_tokens=False\n",
    ").split()\n",
    "plot_attention(\n",
    "    sentence=src_sent,\n",
    "    translation=translation,\n",
    "    attention=attn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = torch.tensor([]).type(torch.long).to(device)\n",
    "Yte = torch.tensor([]).to(device)\n",
    "for data in test_dl:\n",
    "    Xte = torch.cat((Xte, data[0]), dim=0)\n",
    "    Yte = torch.cat((Yte, data[1]), dim=0)\n",
    "Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        src,\n",
    "        model,\n",
    "        src_tok,\n",
    "        tgt_tok,\n",
    "        device,\n",
    "        max_output_length=tgt_tok.max_length,\n",
    "    )[0]\n",
    "    for src in tqdm(Xte)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\" \".join(t.split()[1:-1]) for t in translations]\n",
    "targets = [[\" \".join(tgt_tok.untokenize(t.tolist()).split()[1:-1])] for t in Yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "preds[0], targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=preds, references=targets, tokenizer=lambda x: x.split()\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
