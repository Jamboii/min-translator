{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import Tokenizer, normalize_text\n",
    "import random\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jupyter_black\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "jupyter_black.load()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 512\n"
     ]
    }
   ],
   "source": [
    "src_tok: Tokenizer = torch.load(\"data/src_tok.pt\")\n",
    "tgt_tok: Tokenizer = torch.load(\"data/tgt_tok.pt\")\n",
    "vocab_size = src_tok.vocab_size\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl: DataLoader = torch.load(\"data/train_dl.pt\")\n",
    "val_dl: DataLoader = torch.load(\"data/val_dl.pt\")\n",
    "test_dl: DataLoader = torch.load(\"data/test_dl.pt\")\n",
    "tiny_train_dl: DataLoader = torch.load(\"data/tiny_train_dl.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2sEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(input_size, emb_size)  # (D, V)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param X: (N, T) where N is the batch size and T is the sequence length\n",
    "        :return: (N, T, H) full hidden state tensor where H is the hidden size,\n",
    "        and (L, N, H) final hidden state tensors where L is the number of layers\n",
    "        \"\"\"\n",
    "        emb = self.emb(X)  # (N, T, D)\n",
    "        out, hidden = self.rnn(emb)  # (N, T, H), (L, N, H)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 10]) True\n",
      "torch.Size([1, 5, 10]) True\n"
     ]
    }
   ],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "L = 1  # Number of GRU layers\n",
    "X, _ = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "\n",
    "\"\"\"Testing the Encoder\"\"\"\n",
    "\n",
    "enc = s2sEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    "    num_layers=L,\n",
    ").to(device)\n",
    "out, hidden = enc(X)\n",
    "\n",
    "print(out.shape, out.shape == (*X.shape, H))  # (N, T, H)\n",
    "print(hidden.shape, hidden.shape == (L, X.shape[0], H))  # (L, N, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2sDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.emb = nn.Embedding(output_size, emb_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "        self.lin = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, hidden):\n",
    "        \"\"\"\n",
    "        :param X: (N, T) where N is the batch size and T is the sequence length\n",
    "        :param hidden: final encoder hidden state (L, N, H) where L is the number of layers,\n",
    "        N is the batch size, and H is the hidden size\n",
    "        :return: (N, T, V) where V is the output size\n",
    "        \"\"\"\n",
    "        emb = self.emb(X)  # (N, T, D)\n",
    "        out, hidden = self.rnn(emb, hidden)  # (N, T, H), (L, N, H)\n",
    "        out = self.lin(out)  # (N, T, V)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 10]) True\n",
      "torch.Size([1, 5, 10]) True\n",
      "torch.Size([5, 12, 512]) True\n",
      "torch.Size([1, 5, 10]) True\n"
     ]
    }
   ],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "L = 1  # Number of GRU layers\n",
    "X, Y = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "\n",
    "\"\"\"Testing the Encoder\"\"\"\n",
    "\n",
    "enc = s2sEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    "    num_layers=L,\n",
    ").to(device)\n",
    "out, hidden = enc(X)\n",
    "\n",
    "print(out.shape, out.shape == (*X.shape, H))  # (N, T, H)\n",
    "print(hidden.shape, hidden.shape == (L, X.shape[0], H))  # (L, N, H)\n",
    "\n",
    "\"\"\"Testing the Decoder\"\"\"\n",
    "\n",
    "dec = s2sDecoder(\n",
    "    output_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    "    num_layers=L,\n",
    ").to(device)\n",
    "out, hidden = dec(Y, hidden)\n",
    "\n",
    "print(out.shape, out.shape == (*Y.shape, vocab_size))  # (N, T, V)\n",
    "print(hidden.shape, hidden.shape == (L, Y.shape[0], H))  # (L, N, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: s2sEncoder,\n",
    "        decoder: s2sDecoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        # Initialize uniform weights between -0.08 and 0.08\n",
    "        # for the model\n",
    "        for _, param in module.named_parameters():\n",
    "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "        teacher_force_ratio: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param source: (N, T) input tensor where N is the batch size and T is the sequence length\n",
    "        :param target: (N, T) target tensor\n",
    "        :param teacher_force_ratio: float ratio of teacher forcing (0.0 to 1.0)\n",
    "        :return: (N, T, V) where V is the output size\n",
    "        \"\"\"\n",
    "        N, T = source.shape\n",
    "        V = self.dec.output_size\n",
    "        # Encoder step\n",
    "        _, hidden = self.enc.forward(source)  # (N, T, H), (L, N, H)\n",
    "\n",
    "        # Decoder step\n",
    "        outputs = torch.zeros(N, T, V)  # (N, T, V)\n",
    "        target_t = target[:, :1]  # (N, 1) initial decoder input token\n",
    "\n",
    "        # We loop here as to let the function decide which input to use in each proceeding\n",
    "        # RNN cell\n",
    "        for t in range(1, T):\n",
    "            dec_out, hidden = self.dec.forward(target_t, hidden)  # (N, 1, V), (L, N, H)\n",
    "            # Set decoder output into total outputs\n",
    "            outputs[:, t : t + 1] = dec_out  # (N, 1, V) -> (N, T, V)\n",
    "\n",
    "            # Set up next input to decoder\n",
    "            # If teacher_force_ratio is 0.0, then we use the decoder output as the next input\n",
    "            # If teacher_force_ratio is 1.0, then we use the target as the next input\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            target_t = target[:, t : t + 1] if teacher_force else dec_out.argmax(-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Seq2Seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 512]) True\n"
     ]
    }
   ],
   "source": [
    "D = H = 10  # Embedding and hidden state dimensions\n",
    "L = 1  # Number of GRU layers\n",
    "X, Y = next(iter(tiny_train_dl))  # Unpack just the first source batch (N, T)\n",
    "\n",
    "\"\"\"Testing the Seq2Seq model\"\"\"\n",
    "\n",
    "enc = s2sEncoder(\n",
    "    input_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    "    num_layers=L,\n",
    ")\n",
    "\n",
    "dec = s2sDecoder(\n",
    "    output_size=vocab_size,\n",
    "    emb_size=D,\n",
    "    hidden_size=H,\n",
    "    num_layers=L,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "out = model(X, Y)\n",
    "print(out.shape, out.shape == (*Y.shape, vocab_size))  # (N, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(\n",
    "    model,\n",
    "    source,\n",
    "    target,\n",
    "    loss_fn,\n",
    "    teacher_force_ratio,\n",
    "):\n",
    "    # Forward pass - grab the logits that we'll map\n",
    "    # to probabilities in the loss calculation\n",
    "    logits = model.forward(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        teacher_force_ratio=teacher_force_ratio,\n",
    "    )  # (N, T, V)\n",
    "    _, _, V = logits.shape\n",
    "    # Fit the logits into 2 dimensions\n",
    "    logits = logits[:, 1:].reshape(-1, V).to(device)  # (N*(T-1), V)\n",
    "    target = target[:, 1:].reshape(-1)  # (N*(T-1),)\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_fn(logits, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optim,\n",
    "    teacher_force_ratio: float = 0.5,\n",
    "):\n",
    "    # Iterate through one epoch-worth of data\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through the data loader\n",
    "    for it, batch in enumerate(data_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Unpack the data loader\n",
    "        # into source and target sequences\n",
    "        xb, yb = batch  # (N, T), (N, T)\n",
    "\n",
    "        # Forward pass - grab the logits that we'll map\n",
    "        # to probabilities in the loss calculation\n",
    "        loss = model_forward(model, xb, yb, loss_fn, teacher_force_ratio)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optim.step()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through all data in the data loader\n",
    "    for batch in data_loader:\n",
    "        # Unpack the data loader\n",
    "        xb, yb = batch\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model_forward(model, xb, yb, loss_fn, teacher_force_ratio=0.0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5a551ba12046fc9f582851c78387db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/20)\tTrain Loss:   3.446 | Train PPL:  31.382\tValid Loss:   2.996 | Valid PPL:  19.999\n",
      "(2/20)\tTrain Loss:   2.716 | Train PPL:  15.116\tValid Loss:   2.674 | Valid PPL:  14.505\n",
      "(3/20)\tTrain Loss:   2.439 | Train PPL:  11.463\tValid Loss:   2.667 | Valid PPL:  14.390\n",
      "(4/20)\tTrain Loss:   2.225 | Train PPL:   9.256\tValid Loss:   2.469 | Valid PPL:  11.811\n",
      "(5/20)\tTrain Loss:   2.024 | Train PPL:   7.572\tValid Loss:   2.407 | Valid PPL:  11.100\n",
      "(6/20)\tTrain Loss:   1.853 | Train PPL:   6.381\tValid Loss:   2.275 | Valid PPL:   9.726\n",
      "(7/20)\tTrain Loss:   1.660 | Train PPL:   5.261\tValid Loss:   2.207 | Valid PPL:   9.091\n",
      "(8/20)\tTrain Loss:   1.439 | Train PPL:   4.218\tValid Loss:   2.230 | Valid PPL:   9.298\n",
      "(9/20)\tTrain Loss:   1.273 | Train PPL:   3.572\tValid Loss:   2.233 | Valid PPL:   9.328\n",
      "(10/20)\tTrain Loss:   1.082 | Train PPL:   2.951\tValid Loss:   2.217 | Valid PPL:   9.180\n",
      "(11/20)\tTrain Loss:   1.003 | Train PPL:   2.726\tValid Loss:   2.178 | Valid PPL:   8.827\n",
      "(12/20)\tTrain Loss:   0.816 | Train PPL:   2.261\tValid Loss:   2.117 | Valid PPL:   8.307\n",
      "(13/20)\tTrain Loss:   0.723 | Train PPL:   2.060\tValid Loss:   2.195 | Valid PPL:   8.982\n",
      "(14/20)\tTrain Loss:   0.621 | Train PPL:   1.861\tValid Loss:   2.047 | Valid PPL:   7.747\n",
      "(15/20)\tTrain Loss:   0.481 | Train PPL:   1.617\tValid Loss:   2.132 | Valid PPL:   8.431\n",
      "(16/20)\tTrain Loss:   0.375 | Train PPL:   1.455\tValid Loss:   2.111 | Valid PPL:   8.255\n",
      "(17/20)\tTrain Loss:   0.297 | Train PPL:   1.345\tValid Loss:   2.143 | Valid PPL:   8.528\n",
      "(18/20)\tTrain Loss:   0.260 | Train PPL:   1.297\tValid Loss:   2.137 | Valid PPL:   8.477\n",
      "(19/20)\tTrain Loss:   0.212 | Train PPL:   1.236\tValid Loss:   2.230 | Valid PPL:   9.301\n",
      "(20/20)\tTrain Loss:   0.203 | Train PPL:   1.226\tValid Loss:   2.216 | Valid PPL:   9.170\n"
     ]
    }
   ],
   "source": [
    "D, H, L = 256, 512, 2\n",
    "lr = 1e-3\n",
    "\n",
    "enc = s2sEncoder(vocab_size, D, H, L)\n",
    "dec = s2sDecoder(vocab_size, D, H, L)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_tok.wtoi[tgt_tok.pad_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "teacher_force_ratio = 0.5\n",
    "\n",
    "epochs = 20\n",
    "best_valid_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_dl,\n",
    "        optim=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        teacher_force_ratio=teacher_force_ratio,\n",
    "    )\n",
    "    val_loss = evaluate_epoch(\n",
    "        model=model,\n",
    "        data_loader=val_dl,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    if val_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), \"best-model.pt\")\n",
    "        best_valid_loss = val_loss\n",
    "    print(\n",
    "        f\"({epoch+1}/{epochs})\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\",\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(f\"\\tValid Loss: {val_loss:7.3f} | Valid PPL: {np.exp(val_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.240 | Test PPL:   9.393 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best-model.pt\"))\n",
    "test_loss = evaluate_epoch(model, test_dl, loss_fn)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    src_tokenizer: Tokenizer,\n",
    "    tgt_tokenizer: Tokenizer,\n",
    "    device,\n",
    "    sos_token: str = \"<SOS>\",\n",
    "    eos_token: str = \"<EOS>\",\n",
    "    max_output_length: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    sentence: (T,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sentence = sentence.unsqueeze(0).to(device)  # (1, T)\n",
    "    _, hidden = model.enc(sentence)  # (1, T, D)\n",
    "\n",
    "    X = torch.tensor(\n",
    "        [tgt_tokenizer.wtoi[sos_token]], dtype=torch.long, device=device\n",
    "    ).reshape(1, -1)\n",
    "\n",
    "    for i in range(max_output_length):\n",
    "        dec_out, hidden = model.dec(X, hidden)  # (N, T, V)\n",
    "        logits = dec_out[:, -1]  # (N, V)\n",
    "\n",
    "        pred_token = logits.argmax(-1).reshape(1, -1)\n",
    "        X = torch.cat((X, pred_token), dim=1)  # (N, T+1)\n",
    "\n",
    "        if pred_token.item() == tgt_tokenizer.wtoi[eos_token]:\n",
    "            break\n",
    "\n",
    "    tokens = tgt_tok.untokenize(X.squeeze(0).tolist())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 12])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> ils sont trs <UNK> <EOS>\n",
      "<SOS> they are very kind <EOS>\n"
     ]
    }
   ],
   "source": [
    "ix = 21\n",
    "src, tgt = X[ix], Y[ix]\n",
    "print(src_tok.untokenize(src.tolist()))\n",
    "print(tgt_tok.untokenize(tgt.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> they very <UNK> <EOS>'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = translate_sentence(\n",
    "    src.to(device),\n",
    "    model,\n",
    "    src_tok,\n",
    "    tgt_tok,\n",
    "    device,\n",
    "    max_output_length=tgt_tok.max_length,\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([332, 12]), torch.Size([332, 12]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xte = torch.tensor([]).type(torch.long).to(device)\n",
    "Yte = torch.tensor([]).to(device)\n",
    "for data in test_dl:\n",
    "    Xte = torch.cat((Xte, data[0]), dim=0)\n",
    "    Yte = torch.cat((Yte, data[1]), dim=0)\n",
    "Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5020983853448c9f0d802effeb8432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        src,\n",
    "        model,\n",
    "        src_tok,\n",
    "        tgt_tok,\n",
    "        device,\n",
    "        max_output_length=tgt_tok.max_length,\n",
    "    )\n",
    "    for src in tqdm(Xte)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\" \".join(t.split()[1:-1]) for t in translations]\n",
    "targets = [[\" \".join(tgt_tok.untokenize(t.tolist()).split()[1:-1])] for t in Yte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('he the <UNK> <UNK>', ['he is very <UNK> about his <UNK>'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = -1\n",
    "preds[ix], targets[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.03194757779682639,\n",
       " 'precisions': [0.626362735381566,\n",
       "  0.08862629246676514,\n",
       "  0.025936599423631124,\n",
       "  0.026785714285714284],\n",
       " 'brevity_penalty': 0.4054028021991379,\n",
       " 'length_ratio': 0.5255208333333333,\n",
       " 'translation_length': 1009,\n",
       " 'reference_length': 1920}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=preds, references=targets, tokenizer=lambda x: x.split()\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
